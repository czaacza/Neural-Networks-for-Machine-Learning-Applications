{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 1. Heart Disease Classification\n",
    "**Neural Networks for Machine Learning Applications**<br>\n",
    "23.01.2023<br>\n",
    "## Team X<br>\n",
    "## Amal Kayed\n",
    "## Julian Marco Soliveres\n",
    "## Mateusz Czarnecki\n",
    "[Information Technology, Bachelor's Degree](https://www.metropolia.fi/en/academics/bachelors-degrees/information-technology)<br>\n",
    "[Metropolia University of Applied Sciences](https://www.metropolia.fi/en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "The main objectives of the following notebook is to use neural networks to make an **expert system** to support diagnostic desicion making. We are going to deepen our deep learning knowledge by testing different model architectures, using visualiations tools and metrics. \n",
    "\n",
    "Our task is to firstly read and preprocess the **Heart Disease Health Indicators Dataset** and create a neural network to predict the presence of heart disease among the patients given in the dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup\n",
    "\n",
    "We need several python libraries to make the goal of this assignment possible. \n",
    "\n",
    "The **pandas** and **numpy** libraries allow us to create and operate on DataFrames, read the data from .csv file and perform operations on arrays such as concatinating. \n",
    "\n",
    "The **imblearn** library will become useful when resampling (balancing) the data.\n",
    "\n",
    "The sklearn library is going to help us with **normalizing** the data.\n",
    "\n",
    "We need the **matplotlib** and **seaborn** libraries to visualize the number of specific values in the dataset.\n",
    "\n",
    "The **tensorflow** library is going to help us when building the Machine Learning Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-30T12:48:05.191512Z",
     "iopub.status.busy": "2023-01-30T12:48:05.190917Z",
     "iopub.status.idle": "2023-01-30T12:48:15.346239Z",
     "shell.execute_reply": "2023-01-30T12:48:15.345127Z",
     "shell.execute_reply.started": "2023-01-30T12:48:05.191382Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/gv/mj37zjqs5rb9g8qd98jdzq0r0000gn/T/ipykernel_2013/2788667818.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python3.10 -m pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset consists of **253,680** survey responses collected from The Behavioral Risk Factor Surveillance System 2015. The primary purpose of the dataset is to be used for binary classification of a heart disease. **229,787 of the respondents never had a heart** disease, while **23,893 have or had a heart disease**.\n",
    "\n",
    "A detailed description of the dataset can be found at: https://www.kaggle.com/datasets/alexteboul/heart-disease-health-indicators-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-30T12:48:15.349867Z",
     "iopub.status.busy": "2023-01-30T12:48:15.348619Z",
     "iopub.status.idle": "2023-01-30T12:48:16.418231Z",
     "shell.execute_reply": "2023-01-30T12:48:16.416909Z",
     "shell.execute_reply.started": "2023-01-30T12:48:15.349779Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"heart_disease_health_indicators_BRFSS2015.csv\")\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-30T12:48:16.420309Z",
     "iopub.status.busy": "2023-01-30T12:48:16.419961Z",
     "iopub.status.idle": "2023-01-30T12:48:16.428237Z",
     "shell.execute_reply": "2023-01-30T12:48:16.426644Z",
     "shell.execute_reply.started": "2023-01-30T12:48:16.420279Z"
    }
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions: Describe:\n",
    "\n",
    "- how the missing values are handled\n",
    "- conversion of textual and categorical data into numerical values (if needed)\n",
    "- how the data is splitted into train, validation and test sets\n",
    "- the features (=input) and labels (=output), and \n",
    "- how the features are normalized or scaled\n",
    "\n",
    "At start, let's check if there are any missing values and if our dataset needs a class balance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-30T12:48:16.433417Z",
     "iopub.status.busy": "2023-01-30T12:48:16.432828Z",
     "iopub.status.idle": "2023-01-30T12:48:16.444281Z",
     "shell.execute_reply": "2023-01-30T12:48:16.442855Z",
     "shell.execute_reply.started": "2023-01-30T12:48:16.433368Z"
    }
   },
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start the dataset preprocessing by changing all the values from float to integer values. We don't need the float type because all the numbers are in fact integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-30T12:48:16.446102Z",
     "iopub.status.busy": "2023-01-30T12:48:16.445717Z",
     "iopub.status.idle": "2023-01-30T12:48:16.481479Z",
     "shell.execute_reply": "2023-01-30T12:48:16.480102Z",
     "shell.execute_reply.started": "2023-01-30T12:48:16.446070Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-30T12:48:16.483418Z",
     "iopub.status.busy": "2023-01-30T12:48:16.483067Z",
     "iopub.status.idle": "2023-01-30T12:48:16.493351Z",
     "shell.execute_reply": "2023-01-30T12:48:16.491960Z",
     "shell.execute_reply.started": "2023-01-30T12:48:16.483387Z"
    }
   },
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next thing we are going to do is split the dataframe columns into features (inputs) and labels (outputs). \n",
    "\n",
    "We can also see a big **disproportion** between the disease cases and healthy cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-30T12:48:16.495892Z",
     "iopub.status.busy": "2023-01-30T12:48:16.495388Z",
     "iopub.status.idle": "2023-01-30T12:48:16.580038Z",
     "shell.execute_reply": "2023-01-30T12:48:16.578633Z",
     "shell.execute_reply.started": "2023-01-30T12:48:16.495805Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split column to Features and Target(Y)\n",
    "features = df.drop(columns='HeartDiseaseorAttack')\n",
    "labels = df['HeartDiseaseorAttack']\n",
    "\n",
    "print(f'Disease cases: {sum(labels == 0)}')\n",
    "print(f'Healthy cases: {sum(labels > 0)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make our data more suitable to create an accurate neural network, we need to balance our data We are going to make the amount of disease cases and healthy cases equal using **RandomOverSampler()** function from imblearn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-30T12:48:16.581688Z",
     "iopub.status.busy": "2023-01-30T12:48:16.581358Z",
     "iopub.status.idle": "2023-01-30T12:48:17.025943Z",
     "shell.execute_reply": "2023-01-30T12:48:17.024923Z",
     "shell.execute_reply.started": "2023-01-30T12:48:16.581659Z"
    }
   },
   "outputs": [],
   "source": [
    "random_sampler = RandomOverSampler()\n",
    "features, labels = random_sampler.fit_resample(features, labels)\n",
    "\n",
    "print('Resampled data')\n",
    "print(f'Disease cases: {sum(labels == 0)}')\n",
    "print(f'Healthy cases: {sum(labels > 0)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After seperating the features and outputs and resampling the data, let's study the dataset in more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-30T12:48:17.027346Z",
     "iopub.status.busy": "2023-01-30T12:48:17.027006Z",
     "iopub.status.idle": "2023-01-30T12:48:17.057689Z",
     "shell.execute_reply": "2023-01-30T12:48:17.056425Z",
     "shell.execute_reply.started": "2023-01-30T12:48:17.027317Z"
    }
   },
   "outputs": [],
   "source": [
    "features.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are no null values in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a brief overview on the dataset's statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-30T12:48:17.061777Z",
     "iopub.status.busy": "2023-01-30T12:48:17.061392Z",
     "iopub.status.idle": "2023-01-30T12:48:17.470874Z",
     "shell.execute_reply": "2023-01-30T12:48:17.469539Z",
     "shell.execute_reply.started": "2023-01-30T12:48:17.061744Z"
    }
   },
   "outputs": [],
   "source": [
    "features.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-30T12:48:17.473277Z",
     "iopub.status.busy": "2023-01-30T12:48:17.472601Z",
     "iopub.status.idle": "2023-01-30T12:48:17.481747Z",
     "shell.execute_reply": "2023-01-30T12:48:17.480286Z",
     "shell.execute_reply.started": "2023-01-30T12:48:17.473232Z"
    }
   },
   "outputs": [],
   "source": [
    "features.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-30T12:48:17.483751Z",
     "iopub.status.busy": "2023-01-30T12:48:17.483319Z",
     "iopub.status.idle": "2023-01-30T12:48:26.089971Z",
     "shell.execute_reply": "2023-01-30T12:48:26.088754Z",
     "shell.execute_reply.started": "2023-01-30T12:48:17.483719Z"
    }
   },
   "outputs": [],
   "source": [
    "catcol = ['HighBP', 'HighChol', 'CholCheck', 'BMI',\n",
    "       'Smoker', 'Stroke', 'Diabetes', 'PhysActivity', 'Fruits', 'Veggies',\n",
    "       'HvyAlcoholConsump', 'AnyHealthcare', 'NoDocbcCost', 'GenHlth',\n",
    "       'MentHlth', 'PhysHlth', 'DiffWalk', 'Sex', 'Age', 'Education',\n",
    "       'Income']\n",
    "\n",
    "plt.figure(figsize=(15,40))\n",
    "for i,column in enumerate(catcol):\n",
    "    plt.subplot(len(catcol), 2, i+1)\n",
    "    plt.suptitle(\"Plot Value Count\", fontsize=20, x=0.5, y=1)\n",
    "    sns.countplot(data=features, x=column)\n",
    "    plt.title(f\"{column}\")\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-23T17:31:25.439127Z",
     "iopub.status.busy": "2023-01-23T17:31:25.438639Z",
     "iopub.status.idle": "2023-01-23T17:31:25.446578Z",
     "shell.execute_reply": "2023-01-23T17:31:25.444853Z",
     "shell.execute_reply.started": "2023-01-23T17:31:25.439090Z"
    }
   },
   "source": [
    "Thanks to the statistics, we can clearly divide the features into three sets:\n",
    "* binary features having values 0 or 1\n",
    "* numerical features having a range of values\n",
    "* categorical features having numerical categorical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-30T12:48:26.092380Z",
     "iopub.status.busy": "2023-01-30T12:48:26.091694Z",
     "iopub.status.idle": "2023-01-30T12:48:26.100225Z",
     "shell.execute_reply": "2023-01-30T12:48:26.098392Z",
     "shell.execute_reply.started": "2023-01-30T12:48:26.092338Z"
    }
   },
   "outputs": [],
   "source": [
    "bin_features = ['HighBP', 'HighChol', 'CholCheck', 'Smoker', 'Stroke', \n",
    "                'PhysActivity', 'Fruits', 'Veggies', 'HvyAlcoholConsump',\n",
    "               'AnyHealthcare', 'NoDocbcCost', 'DiffWalk', 'Sex']\n",
    "\n",
    "num_features = ['BMI','MentHlth', 'PhysHlth']\n",
    "\n",
    "cat_features = ['Diabetes', 'GenHlth', 'Age', 'Education', 'Income']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the binary features can be easily merged together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-30T12:48:26.102991Z",
     "iopub.status.busy": "2023-01-30T12:48:26.101801Z",
     "iopub.status.idle": "2023-01-30T12:48:26.134770Z",
     "shell.execute_reply": "2023-01-30T12:48:26.133492Z",
     "shell.execute_reply.started": "2023-01-30T12:48:26.102922Z"
    }
   },
   "outputs": [],
   "source": [
    "bin_values = features[bin_features].values\n",
    "bin_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the numerical values, we will use **RobustScaler()** function from the preprocessing library, which normalizes the numerical values and makes them easier to analyze by our future model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-30T12:48:26.137159Z",
     "iopub.status.busy": "2023-01-30T12:48:26.136597Z",
     "iopub.status.idle": "2023-01-30T12:48:26.217610Z",
     "shell.execute_reply": "2023-01-30T12:48:26.216238Z",
     "shell.execute_reply.started": "2023-01-30T12:48:26.137094Z"
    }
   },
   "outputs": [],
   "source": [
    "transformer = preprocessing.RobustScaler().fit(features[num_features])\n",
    "num_values = transformer.transform(features[num_features])\n",
    "\n",
    "num_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the categorical values, we will use a **OneHotEncoder()** function from the preprocessing library, which changes their numerical values into arrays representing a given category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-30T12:48:26.219499Z",
     "iopub.status.busy": "2023-01-30T12:48:26.219044Z",
     "iopub.status.idle": "2023-01-30T12:48:26.711941Z",
     "shell.execute_reply": "2023-01-30T12:48:26.710752Z",
     "shell.execute_reply.started": "2023-01-30T12:48:26.219466Z"
    }
   },
   "outputs": [],
   "source": [
    "encoder = preprocessing.OneHotEncoder().fit(features[cat_features])\n",
    "cat_values = encoder.transform(features[cat_features]).toarray()\n",
    "\n",
    "cat_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After normalizing our data, we have to concatenate all the features together into one DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-30T12:48:26.714267Z",
     "iopub.status.busy": "2023-01-30T12:48:26.713660Z",
     "iopub.status.idle": "2023-01-30T12:48:26.834127Z",
     "shell.execute_reply": "2023-01-30T12:48:26.832480Z",
     "shell.execute_reply.started": "2023-01-30T12:48:26.714218Z"
    }
   },
   "outputs": [],
   "source": [
    "all_values = np.concatenate((bin_values, num_values, cat_values), axis = 1)\n",
    "features = pd.DataFrame(all_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how our features look like after normalizing the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-30T12:48:26.836059Z",
     "iopub.status.busy": "2023-01-30T12:48:26.835685Z",
     "iopub.status.idle": "2023-01-30T12:48:27.096255Z",
     "shell.execute_reply": "2023-01-30T12:48:27.094995Z",
     "shell.execute_reply.started": "2023-01-30T12:48:26.836026Z"
    }
   },
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step of the preprocessing is to divide the features and labels into training, testing and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-30T12:48:27.098130Z",
     "iopub.status.busy": "2023-01-30T12:48:27.097707Z",
     "iopub.status.idle": "2023-01-30T12:48:27.359496Z",
     "shell.execute_reply": "2023-01-30T12:48:27.358320Z",
     "shell.execute_reply.started": "2023-01-30T12:48:27.098095Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data, test_data, train_labels, test_labels = train_test_split(features, labels, test_size=0.2, stratify=labels, random_state=48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-30T12:48:27.361309Z",
     "iopub.status.busy": "2023-01-30T12:48:27.360950Z",
     "iopub.status.idle": "2023-01-30T12:48:27.825107Z",
     "shell.execute_reply": "2023-01-30T12:48:27.824186Z",
     "shell.execute_reply.started": "2023-01-30T12:48:27.361277Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data, val_data, train_labels, val_labels = train_test_split(train_data, train_labels, test_size=0.2, stratify=train_labels, random_state=48)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After using the **train_test_split()** function two times, according to the 80/20 rule, we receive the data split into training, validation and testing sets. As a result we get the proportions:\n",
    "* Training set   - 64%\n",
    "* Validation set - 16%\n",
    "* Testing set    - 20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-30T12:48:27.828592Z",
     "iopub.status.busy": "2023-01-30T12:48:27.828102Z",
     "iopub.status.idle": "2023-01-30T12:48:27.838594Z",
     "shell.execute_reply": "2023-01-30T12:48:27.837255Z",
     "shell.execute_reply.started": "2023-01-30T12:48:27.828545Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data = train_data.to_numpy()\n",
    "train_labels = train_labels.to_numpy()\n",
    "\n",
    "val_data = val_data.to_numpy()\n",
    "val_labels = val_labels.to_numpy()\n",
    "\n",
    "test_data = test_data.to_numpy()\n",
    "test_labels = test_labels.to_numpy()\n",
    "\n",
    "# Counting the data %\n",
    "sum_length = len(train_data) + len(val_data) + len(test_data)\n",
    "train_percent =  len(train_data) / sum_length * 100\n",
    "val_percent = len(val_data) / sum_length * 100\n",
    "test_percent = len(test_data) / sum_length * 100\n",
    "\n",
    "print('train data %:\\t',train_percent)\n",
    "print('val data %:\\t', val_percent)\n",
    "print('test data %:\\t', test_percent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions: Write a short description of the model: \n",
    "\n",
    "- selected loss, optimizer and metrics settings, and \n",
    "- the summary of the selected model architecture. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-30T12:48:27.840458Z",
     "iopub.status.busy": "2023-01-30T12:48:27.840075Z",
     "iopub.status.idle": "2023-01-30T12:48:27.850737Z",
     "shell.execute_reply": "2023-01-30T12:48:27.849399Z",
     "shell.execute_reply.started": "2023-01-30T12:48:27.840426Z"
    }
   },
   "outputs": [],
   "source": [
    "print(train_data.shape, train_data.dtype)\n",
    "print(train_labels.shape, train_labels.dtype)\n",
    "print(val_data.shape, val_data.dtype)\n",
    "print(val_labels.shape, val_labels.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-30T13:00:57.873925Z",
     "iopub.status.busy": "2023-01-30T13:00:57.873481Z",
     "iopub.status.idle": "2023-01-30T13:00:57.908600Z",
     "shell.execute_reply": "2023-01-30T13:00:57.907227Z",
     "shell.execute_reply.started": "2023-01-30T13:00:57.873886Z"
    }
   },
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(5, activation='relu', input_shape=(51,))) # input_shape=(51,)\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions: Write a short description of the training process, and document the code for training and the total time spend on it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-30T12:59:52.787622Z",
     "iopub.status.busy": "2023-01-30T12:59:52.787198Z",
     "iopub.status.idle": "2023-01-30T13:00:29.720570Z",
     "shell.execute_reply": "2023-01-30T13:00:29.719276Z",
     "shell.execute_reply.started": "2023-01-30T12:59:52.787590Z"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['Accuracy','Recall'])\n",
    "\n",
    "hist = model.fit(train_data, train_labels, epochs=1, batch_size=16,\n",
    "                 validation_data = (val_data, val_labels),verbose=0)\n",
    "\n",
    "print('Accuracy:',hist.history['Accuracy'][-1])\n",
    "print('Recall:',hist.history['recall'][-1])\n",
    "print('Val Accuracy:',hist.history['val_Accuracy'][-1])\n",
    "print('Val Recall:',hist.history['val_recall'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-30T12:49:00.542071Z",
     "iopub.status.busy": "2023-01-30T12:49:00.541541Z",
     "iopub.status.idle": "2023-01-30T12:49:00.762862Z",
     "shell.execute_reply": "2023-01-30T12:49:00.761500Z",
     "shell.execute_reply.started": "2023-01-30T12:49:00.542030Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(hist.history['loss'], label='Loss')\n",
    "plt.plot(hist.history['val_loss'], label='Val loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss value')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-30T12:49:00.765015Z",
     "iopub.status.busy": "2023-01-30T12:49:00.764553Z",
     "iopub.status.idle": "2023-01-30T12:49:00.987911Z",
     "shell.execute_reply": "2023-01-30T12:49:00.986916Z",
     "shell.execute_reply.started": "2023-01-30T12:49:00.764973Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(hist.history['Accuracy'], label='Acc')\n",
    "plt.plot(hist.history['val_Accuracy'], label='Val acc')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Performance and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions: \n",
    "\n",
    "- Show the training and validation loss and accuracy plots\n",
    "- Interpret the loss and accuracy plots (e.g. is there under- or over-fitting)\n",
    "- Describe the final performance of the model with test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-30T12:49:00.990184Z",
     "iopub.status.busy": "2023-01-30T12:49:00.989434Z",
     "iopub.status.idle": "2023-01-30T12:49:00.996208Z",
     "shell.execute_reply": "2023-01-30T12:49:00.994540Z",
     "shell.execute_reply.started": "2023-01-30T12:49:00.990132Z"
    }
   },
   "outputs": [],
   "source": [
    "# Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Discussion and conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions: Write\n",
    "\n",
    "- What settings and models were tested before the best model was found\n",
    "    - What where the results of these experiments \n",
    "- Summary of  \n",
    "    - What was your best model and its settings \n",
    "    - What was the final achieved performance \n",
    "- What are your main observations and learning points\n",
    "- Discussion how the model could be improved in future \n",
    "\n",
    "**Note:** Remember to evaluate the final metrics using the test set. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
